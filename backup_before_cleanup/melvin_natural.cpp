#include <iostream>
#include <string>
#include <vector>
#include <map>
#include <unordered_map>
#include <chrono>
#include <thread>
#include <mutex>
#include <memory>
#include <algorithm>
#include <cstring>
#include <cstdint>
#include <set>
#include <optional>
#include <sstream>
#include <fstream>
#include <filesystem>
#include <iomanip>
#include <random>
#include <atomic>
#include <limits>
#include <ctime>
#include <cstdlib>
#include <curl/curl.h>

// ============================================================================
// UPGRADED MELVIN UNIFIED BRAIN WITH NATURAL RESPONSES
// ============================================================================

// Curl write callback function
size_t write_callback(void* contents, size_t size, size_t nmemb, std::string* s) {
    size_t newLength = size * nmemb;
    try {
        s->append((char*)contents, newLength);
        return newLength;
    } catch (std::bad_alloc& e) {
        return static_cast<size_t>(0);
    }
}

enum class ContentType : uint8_t {
    TEXT = 0, CONCEPT = 1, REASONING = 2, MEMORY = 3, RESEARCH = 4
};

enum class ConnectionType : uint8_t {
    HEBBIAN = 0, SEMANTIC = 1, TEMPORAL = 2, ASSOCIATIVE = 3
};

enum class InstinctType : uint8_t {
    SURVIVAL = 0, CURIOSITY = 1, EFFICIENCY = 2, SOCIAL = 3, CONSISTENCY = 4
};

struct BinaryNode {
    uint64_t id;
    uint64_t creation_time;
    ContentType content_type;
    uint8_t importance;
    uint8_t instinct_bias;
    uint32_t content_length;
    uint32_t connection_count;
    std::string content;
    
    BinaryNode() : id(0), creation_time(0), content_type(ContentType::TEXT),
                   importance(0), instinct_bias(0), content_length(0), connection_count(0) {}
};

struct BinaryConnection {
    uint64_t id;
    uint64_t source_id;
    uint64_t target_id;
    ConnectionType connection_type;
    uint8_t weight;
    
    BinaryConnection() : id(0), source_id(0), target_id(0),
                        connection_type(ConnectionType::HEBBIAN), weight(0) {}
};

struct InstinctBias {
    float recall_weight;
    float exploration_weight;
    std::map<InstinctType, float> instinct_contributions;
    std::string reasoning;
    float overall_strength;
    
    InstinctBias() : recall_weight(0.5f), exploration_weight(0.5f), overall_strength(0.0f) {}
};

struct ResearchResult {
    std::string query;
    std::vector<std::string> findings;
    bool success;
    std::string source;
    
    ResearchResult() : success(false) {}
};

struct AnswerComponents {
    std::vector<std::string> recall_facts;
    std::vector<std::string> hypotheses;
    std::vector<std::string> research_findings;
    float confidence;
    InstinctBias instinct_bias;
    
    AnswerComponents() : confidence(0.0f) {}
};

class MelvinUnifiedBrain {
private:
    std::unordered_map<uint64_t, BinaryNode> nodes;
    std::unordered_map<uint64_t, BinaryConnection> connections;
    std::mutex storage_mutex;
    
    uint64_t next_node_id;
    uint64_t next_connection_id;
    
    std::map<InstinctType, float> instinct_weights;
    std::mutex instinct_mutex;
    
    std::map<std::pair<uint64_t, uint64_t>, float> hebbian_weights;
    std::mutex hebbian_mutex;
    
    // Knowledge base for better responses
    std::map<std::string, std::string> knowledge_base;
    
    // Global Memory System
    std::string memory_directory;
    std::mutex memory_mutex;
    bool memory_enabled;
    
    bool debug_mode;
    
    struct BrainStats {
        uint64_t total_nodes;
        uint64_t total_connections;
        uint64_t hebbian_updates;
        uint64_t reasoning_paths;
        uint64_t web_searches;
        double total_processing_time;
    } stats;
    
public:
    MelvinUnifiedBrain(bool debug = false) : next_node_id(1), next_connection_id(1), debug_mode(debug) {
        // Initialize global memory system
        memory_directory = "melvin_binary_memory";
        memory_enabled = true;
        
        // Create memory directory if it doesn't exist
        std::filesystem::create_directories(memory_directory);
        
        // Load existing brain state from global memory
        load_global_memory();
        
        initialize_instincts();
        initialize_knowledge_base();
        
        if (debug_mode) {
            std::cout << "ðŸ” DuckDuckGo web search enabled (no API key required)!" << std::endl;
            std::cout << "ðŸ’¾ Global memory system enabled - brain state persists across sessions!" << std::endl;
        }
        
        stats.total_nodes = nodes.size();
        stats.total_connections = connections.size();
        stats.hebbian_updates = 0;
        stats.reasoning_paths = 0;
        stats.web_searches = 0;
        stats.total_processing_time = 0.0;
        
        if (debug_mode) {
            std::cout << "ðŸ§  Melvin Unified Brain initialized!" << std::endl;
            std::cout << "ðŸ“Š Loaded " << stats.total_nodes << " nodes and " << stats.total_connections << " connections from global memory" << std::endl;
        }
    }
    
    ~MelvinUnifiedBrain() {
        // Save brain state to global memory on shutdown
        if (memory_enabled) {
            save_global_memory();
            if (debug_mode) {
                std::cout << "ðŸ’¾ Brain state saved to global memory on shutdown" << std::endl;
            }
        }
    }
    
    void initialize_instincts() {
        std::lock_guard<std::mutex> lock(instinct_mutex);
        
        // Enhanced curiosity-driven weights
        instinct_weights[InstinctType::SURVIVAL] = 0.8f;
        instinct_weights[InstinctType::CURIOSITY] = 0.9f; // Increased curiosity
        instinct_weights[InstinctType::EFFICIENCY] = 0.4f; // Reduced efficiency to allow more exploration
        instinct_weights[InstinctType::SOCIAL] = 0.6f;
        instinct_weights[InstinctType::CONSISTENCY] = 0.7f;
    }
    
    void initialize_knowledge_base() {
        // Initialize with some basic knowledge for better responses
        knowledge_base["cancer"] = "Cancer is a group of diseases characterized by uncontrolled cell growth. It can affect any part of the body and occurs when cells divide uncontrollably and spread into surrounding tissues.";
        knowledge_base["artificial intelligence"] = "Artificial Intelligence (AI) is the simulation of human intelligence in machines that are programmed to think and learn like humans. It includes machine learning, natural language processing, and robotics.";
        knowledge_base["quantum computing"] = "Quantum computing uses quantum mechanical phenomena like superposition and entanglement to process information. Unlike classical computers that use bits, quantum computers use quantum bits (qubits).";
        knowledge_base["machine learning"] = "Machine learning is a subset of artificial intelligence that enables computers to learn and improve from experience without being explicitly programmed.";
        knowledge_base["hello"] = "Hello! I'm Melvin, your AI companion with a unified brain system.";
        knowledge_base["melvin"] = "I'm Melvin, an AI with a unified brain system that stores information as binary nodes and creates connections through Hebbian learning.";
        knowledge_base["brain"] = "A brain is an organ that serves as the center of the nervous system. In my case, I have a unified brain system that processes information through binary nodes and connections.";
        knowledge_base["memory"] = "Memory is the faculty by which the mind stores and remembers information. My memory system uses binary nodes with compressed storage and Hebbian learning connections.";
        knowledge_base["learning"] = "Learning is the acquisition of knowledge or skills through experience, study, or teaching. I learn through Hebbian learning, where connections between co-activated nodes strengthen over time.";
        knowledge_base["instinct"] = "Instincts are innate patterns of behavior. I have five core instincts: Survival, Curiosity, Efficiency, Social, and Consistency that guide my reasoning and responses.";
    }
    
    uint64_t store_node(const std::string& content, ContentType type, uint8_t importance = 128) {
        std::lock_guard<std::mutex> lock(storage_mutex);
        
        BinaryNode node;
        node.id = next_node_id++;
        node.creation_time = std::chrono::duration_cast<std::chrono::milliseconds>(
            std::chrono::system_clock::now().time_since_epoch()).count();
        node.content_type = type;
        node.importance = importance;
        node.instinct_bias = 0;
        node.content_length = content.length();
        node.connection_count = 0;
        node.content = content;
        
        nodes[node.id] = node;
        stats.total_nodes++;
        
        // Auto-save to global memory
        auto_save_memory();
        
        if (debug_mode) {
            std::cout << "ðŸ’¾ Stored node " << node.id << " (" << content.substr(0, 50) 
                      << "...) with importance " << static_cast<int>(importance) << std::endl;
        }
        
        return node.id;
    }
    
    uint64_t store_connection(uint64_t source_id, uint64_t target_id, ConnectionType type, uint8_t weight = 128) {
        std::lock_guard<std::mutex> lock(storage_mutex);
        
        BinaryConnection conn;
        conn.id = next_connection_id++;
        conn.source_id = source_id;
        conn.target_id = target_id;
        conn.connection_type = type;
        conn.weight = weight;
        
        connections[conn.id] = conn;
        stats.total_connections++;
        
        // Auto-save to global memory
        auto_save_memory();
        
        if (debug_mode) {
            std::cout << "ðŸ”— Stored connection " << conn.id << " (" << source_id 
                      << " â†’ " << target_id << ") with weight " << static_cast<int>(weight) << std::endl;
        }
        
        return conn.id;
    }
    
    std::string get_node_content(uint64_t node_id) {
        std::lock_guard<std::mutex> lock(storage_mutex);
        
        auto it = nodes.find(node_id);
        if (it != nodes.end()) {
            return it->second.content;
        }
        return "";
    }
    
    std::string process_input(const std::string& user_input) {
        auto start_time = std::chrono::high_resolution_clock::now();
        
        if (debug_mode) {
            std::cout << "\nðŸ§  [Unified Brain] Processing: \"" << user_input << "\"" << std::endl;
        }
        
        // Step 1: Parse input to activations
        std::vector<uint64_t> activations = parse_to_activations(user_input);
        if (debug_mode) {
            std::cout << "ðŸ“ [Parse] Activated " << activations.size() << " nodes" << std::endl;
        }
        
        // Step 2: Recall related nodes
        std::vector<uint64_t> recalled_nodes = recall_related_nodes(activations);
        if (debug_mode) {
            std::cout << "ðŸ§  [Recall] Retrieved " << recalled_nodes.size() << " related nodes" << std::endl;
        }
        
        // Step 3: Generate hypotheses
        std::vector<std::string> hypotheses = generate_hypotheses(recalled_nodes);
        if (debug_mode) {
            std::cout << "ðŸ’¡ [Hypotheses] Generated " << hypotheses.size() << " hypotheses" << std::endl;
        }
        
        // Step 4: Calculate confidence and trigger research if needed
        float confidence = calculate_confidence(recalled_nodes, hypotheses);
        ResearchResult research_result;
        
        if (should_trigger_curiosity(recalled_nodes, confidence)) {
            if (debug_mode) {
                std::cout << "ðŸ” [Curiosity] Low confidence (" << confidence 
                          << ") - triggering knowledge research" << std::endl;
            }
            research_result = perform_knowledge_research(user_input);
            
            // Enhanced curiosity: Also research related concepts
            if (research_result.success && activations.size() > 1) {
                for (size_t i = 0; i < std::min(activations.size(), size_t(3)); ++i) {
                    auto node_it = nodes.find(activations[i]);
                    if (node_it != nodes.end()) {
                        std::string related_query = node_it->second.content + " " + user_input;
                        if (debug_mode) {
                            std::cout << "ðŸ” [Enhanced Curiosity] Researching related: " << related_query << std::endl;
                        }
                        ResearchResult related_result = perform_knowledge_research(related_query);
                        if (related_result.success) {
                            // Merge findings
                            research_result.findings.insert(research_result.findings.end(), 
                                                         related_result.findings.begin(), 
                                                         related_result.findings.end());
                        }
                    }
                }
            }
        }
        
        // Step 5: Update Hebbian learning with research integration
        std::vector<uint64_t> all_nodes = activations;
        all_nodes.insert(all_nodes.end(), recalled_nodes.begin(), recalled_nodes.end());
        
        // Add research findings to the learning network
        if (research_result.success) {
            for (const auto& finding : research_result.findings) {
                uint64_t research_node_id = store_node(finding, ContentType::RESEARCH, 150);
                if (research_node_id > 0) {
                    all_nodes.push_back(research_node_id);
                    
                    // Create strong connections between research and original query
                    for (uint64_t activation : activations) {
                        create_or_strengthen_connection(activation, research_node_id, 75.0f);
                    }
                }
            }
        }
        
        update_hebbian_learning(all_nodes);
        
        // Step 6: Adjust instinct weights based on learning success
        float confidence_gain = research_result.success ? 0.3f : 0.0f;
        adjust_instinct_weights_based_on_learning(research_result.success, confidence_gain);
        
        // Step 7: Get instinct bias
        InstinctBias bias = get_instinct_bias(user_input, all_nodes);
        
        // Step 8: Synthesize natural answer
        AnswerComponents components;
        components.recall_facts = extract_recall_facts(recalled_nodes);
        components.hypotheses = hypotheses;
        components.research_findings = research_result.findings;
        components.confidence = confidence;
        components.instinct_bias = bias;
        
        std::string final_response = synthesize_natural_answer(user_input, components);
        
        auto end_time = std::chrono::high_resolution_clock::now();
        auto duration = std::chrono::duration_cast<std::chrono::milliseconds>(end_time - start_time);
        stats.total_processing_time += duration.count();
        stats.reasoning_paths++;
        
        if (debug_mode) {
            std::cout << "â±ï¸ [Timing] Processed in " << duration.count() << "ms" << std::endl;
        }
        
        // Proactive learning: Fill knowledge gaps every few interactions (much reduced frequency)
        static int interaction_count = 0;
        interaction_count++;
        if (interaction_count % 20 == 0) { // Every 20th interaction (much reduced)
            if (debug_mode) {
                std::cout << "ðŸ§  [Proactive Learning] Analyzing knowledge gaps..." << std::endl;
            }
            // Only do proactive learning if we have very few nodes to avoid explosion
            if (stats.total_nodes < 50) {
                proactive_knowledge_gap_analysis();
            }
        }
        
        return final_response;
    }
    
    std::vector<uint64_t> parse_to_activations(const std::string& input) {
        std::vector<uint64_t> activations;
        std::vector<std::string> tokens = tokenize(input);
        
        for (const auto& token : tokens) {
            // Check if token exists as a node
            bool found = false;
            for (const auto& [node_id, node] : nodes) {
                if (node.content.find(token) != std::string::npos) {
                    activations.push_back(node_id);
                    found = true;
                    break;
                }
            }
            
            // If no existing node found, create new one
            if (!found) {
                uint64_t new_node_id = store_node(token, ContentType::TEXT, 128);
                if (new_node_id > 0) {
                    activations.push_back(new_node_id);
                }
            }
        }
        
        return activations;
    }
    
    std::vector<uint64_t> recall_related_nodes(const std::vector<uint64_t>& activations) {
        std::vector<uint64_t> related_nodes;
        std::set<uint64_t> visited;
        
        for (uint64_t activation : activations) {
            if (visited.find(activation) != visited.end()) continue;
            visited.insert(activation);
            
            // Find connections from this node
            for (const auto& [conn_id, conn] : connections) {
                if (conn.source_id == activation) {
                    related_nodes.push_back(conn.target_id);
                }
            }
        }
        
        return related_nodes;
    }
    
    std::vector<std::string> generate_hypotheses(const std::vector<uint64_t>& nodes) {
        std::vector<std::string> hypotheses;
        
        if (nodes.size() >= 2) {
            hypotheses.push_back("Multiple concepts detected - exploring relationships");
            hypotheses.push_back("Pattern recognition suggests deeper connections");
        } else if (nodes.size() == 1) {
            hypotheses.push_back("Single concept - seeking related information");
        } else {
            hypotheses.push_back("No clear patterns - requires external knowledge");
        }
        
        return hypotheses;
    }
    
    float calculate_confidence(const std::vector<uint64_t>& nodes, const std::vector<std::string>& hypotheses) {
        float base_confidence = nodes.empty() ? 0.2f : 
                               (nodes.size() < 3 ? 0.4f : 0.7f);
        
        // Adjust based on hypothesis quality
        if (hypotheses.size() > 1) {
            base_confidence += 0.1f;
        }
        
        return std::min(1.0f, base_confidence);
    }
    
    bool should_trigger_curiosity(const std::vector<uint64_t>& nodes, float confidence) {
        // Enhanced curiosity triggers
        if (confidence < 0.6f) return true; // Lower threshold for more curiosity
        
        // Trigger curiosity if we have few related nodes (knowledge gap)
        if (nodes.size() < 3) return true;
        
        // Trigger curiosity for complex topics that need research
        return confidence < 0.7f;
    }
    
    void proactive_knowledge_gap_analysis() {
        // Analyze knowledge gaps and trigger curiosity-driven learning
        std::vector<std::string> gap_topics;
        
        // Check for common knowledge gaps
        std::vector<std::string> common_topics = {
            "artificial intelligence", "machine learning", "quantum computing",
            "climate change", "space exploration", "neuroscience", "philosophy",
            "history", "science", "technology", "medicine", "psychology"
        };
        
        for (const auto& topic : common_topics) {
            std::vector<uint64_t> topic_nodes = parse_to_activations(topic);
            if (topic_nodes.size() < 2) { // Knowledge gap detected
                gap_topics.push_back(topic);
            }
        }
        
        // Proactively research gaps
        for (const auto& gap : gap_topics) {
            if (debug_mode) {
                std::cout << "ðŸ” [Proactive Learning] Filling knowledge gap: " << gap << std::endl;
            }
            ResearchResult result = perform_knowledge_research(gap);
            if (result.success) {
                // Store the research as background knowledge
                for (const auto& finding : result.findings) {
                    store_node(finding, ContentType::RESEARCH, 140);
                }
            }
        }
    }
    
    void adjust_instinct_weights_based_on_learning(bool research_successful, float confidence_gain) {
        std::lock_guard<std::mutex> lock(instinct_mutex);
        
        if (research_successful && confidence_gain > 0.1f) {
            // Successful learning - increase curiosity, decrease efficiency
            instinct_weights[InstinctType::CURIOSITY] = std::min(1.0f, instinct_weights[InstinctType::CURIOSITY] + 0.05f);
            instinct_weights[InstinctType::EFFICIENCY] = std::max(0.2f, instinct_weights[InstinctType::EFFICIENCY] - 0.02f);
            
            if (debug_mode) {
                std::cout << "ðŸ§  [Instinct Learning] Curiosity increased, efficiency decreased" << std::endl;
            }
        } else if (!research_successful) {
            // Failed research - slightly increase efficiency
            instinct_weights[InstinctType::EFFICIENCY] = std::min(0.8f, instinct_weights[InstinctType::EFFICIENCY] + 0.01f);
        }
    }
    
    ResearchResult perform_knowledge_research(const std::string& query) {
        ResearchResult result;
        result.query = query;
        stats.web_searches++;
        
        // First try knowledge base
        std::string lower_query = query;
        std::transform(lower_query.begin(), lower_query.end(), lower_query.begin(), ::tolower);
        
        for (const auto& [key, value] : knowledge_base) {
            if (lower_query.find(key) != std::string::npos) {
                result.findings.push_back(value);
                result.success = true;
                result.source = "Knowledge Base";
                
                // Store research result as a node
                store_node(value, ContentType::RESEARCH, 150);
                return result;
            }
        }
        
        // Try DuckDuckGo API (no authentication required!)
        if (debug_mode) {
            std::cout << "ðŸ” [Research] Searching DuckDuckGo for: " << query << std::endl;
        }
        result = perform_duckduckgo_search(query);
        
        // If DuckDuckGo fails, try with simplified query
        if (!result.success || result.findings.empty()) {
            if (debug_mode) {
                std::cout << "ðŸ” [Research] DuckDuckGo failed, trying simplified query..." << std::endl;
            }
            
            // Extract key words from query
            std::string simplified_query = extract_keywords(query);
            if (!simplified_query.empty() && simplified_query != query) {
                result = perform_duckduckgo_search(simplified_query);
                if (debug_mode) {
                    std::cout << "ðŸ” [Research] Simplified search result: " << (result.success ? "SUCCESS" : "FAILED") 
                              << " (" << result.findings.size() << " findings)" << std::endl;
                }
            }
        }
        
        if (debug_mode) {
            std::cout << "ðŸ” [Research] Final DuckDuckGo result: " << (result.success ? "SUCCESS" : "FAILED") 
                      << " (" << result.findings.size() << " findings)" << std::endl;
        }
        if (result.success) {
            return result;
        }
        
        // Fallback to general response
        std::string general_response = generate_general_response(query);
        result.findings.push_back(general_response);
        result.success = true;
        result.source = "General Response";
        
        // Store general response as a node
        store_node(general_response, ContentType::RESEARCH, 120);
        return result;
    }
    
    ResearchResult perform_duckduckgo_search(const std::string& query) {
        ResearchResult result;
        result.query = query;
        
        if (debug_mode) {
            std::cout << "ðŸŒ [DuckDuckGo] Starting search for: " << query << std::endl;
        }
        
        // Initialize libcurl
        CURL* curl = curl_easy_init();
        if (!curl) {
            if (debug_mode) {
                std::cout << "âŒ [DuckDuckGo] Failed to initialize curl" << std::endl;
            }
            result.findings.push_back("Failed to initialize web search");
            result.success = false;
            return result;
        }
        
        std::string readBuffer;
        
        // Construct DuckDuckGo API URL (no authentication required!)
        std::string encoded_query = query;
        std::replace(encoded_query.begin(), encoded_query.end(), ' ', '+');
        std::string url = "https://api.duckduckgo.com/?q=" + encoded_query + "&format=json&no_html=1&skip_disambig=1&t=melvin";
        
        if (debug_mode) {
            std::cout << "ðŸŒ [DuckDuckGo] URL: " << url << std::endl;
        }
        
        // Set up curl options
        curl_easy_setopt(curl, CURLOPT_URL, url.c_str());
        curl_easy_setopt(curl, CURLOPT_WRITEFUNCTION, write_callback);
        curl_easy_setopt(curl, CURLOPT_WRITEDATA, &readBuffer);
        curl_easy_setopt(curl, CURLOPT_TIMEOUT, 10L); // 10 second timeout
        curl_easy_setopt(curl, CURLOPT_FOLLOWLOCATION, 1L); // Follow redirects
        curl_easy_setopt(curl, CURLOPT_SSL_VERIFYPEER, 0L); // Skip SSL verification for testing
        curl_easy_setopt(curl, CURLOPT_USERAGENT, "Melvin-Brain/1.0"); // Set user agent
        curl_easy_setopt(curl, CURLOPT_HTTP_VERSION, CURL_HTTP_VERSION_1_1); // Use HTTP/1.1
        curl_easy_setopt(curl, CURLOPT_ACCEPT_ENCODING, ""); // Accept any encoding
        
        // Perform request
        CURLcode res = curl_easy_perform(curl);
        
        if (debug_mode) {
            std::cout << "ðŸŒ [DuckDuckGo] HTTP result: " << curl_easy_strerror(res) << std::endl;
            std::cout << "ðŸŒ [DuckDuckGo] Response length: " << readBuffer.length() << " bytes" << std::endl;
            if (readBuffer.length() < 2000) {
                std::cout << "ðŸŒ [DuckDuckGo] Response: " << readBuffer << std::endl;
            }
        }
        
        // Cleanup
        curl_easy_cleanup(curl);
        
        if (res != CURLE_OK) {
            if (debug_mode) {
                std::cout << "âŒ [DuckDuckGo] HTTP error: " << curl_easy_strerror(res) << std::endl;
            }
            result.findings.push_back("Web search failed: " + std::string(curl_easy_strerror(res)));
            result.success = false;
            return result;
        }
        
        // Parse DuckDuckGo JSON response
        result = parse_duckduckgo_response(readBuffer, query);
        
        if (debug_mode) {
            std::cout << "ðŸŒ [DuckDuckGo] Parsed " << result.findings.size() << " findings" << std::endl;
        }
        
        return result;
    }
    
    ResearchResult parse_duckduckgo_response(const std::string& response, const std::string& query) {
        ResearchResult result;
        result.query = query;
        
        if (debug_mode) {
            std::cout << "ðŸ” [Parse] Parsing DuckDuckGo response..." << std::endl;
        }
        
        // Parse DuckDuckGo JSON response
        // DuckDuckGo returns: Abstract, Answer, Definition, RelatedTopics, Results
        
        // Check for Abstract (main answer)
        size_t abstract_pos = response.find("\"Abstract\":\"");
        if (debug_mode) {
            std::cout << "ðŸ” [Parse] Looking for Abstract at position: " << abstract_pos << std::endl;
        }
        if (abstract_pos != std::string::npos) {
            size_t abstract_start = abstract_pos + 12; // Skip "Abstract":"
            size_t abstract_end = response.find("\"", abstract_start);
            if (abstract_end != std::string::npos && abstract_end > abstract_start) {
                std::string abstract = response.substr(abstract_start, abstract_end - abstract_start);
                if (!abstract.empty() && abstract != "") {
                    if (debug_mode) {
                        std::cout << "ðŸ” [Parse] Found Abstract: " << abstract.substr(0, 100) << "..." << std::endl;
                    }
                    result.findings.push_back("Summary: " + abstract);
                    store_node(abstract, ContentType::RESEARCH, 150);
                } else if (debug_mode) {
                    std::cout << "ðŸ” [Parse] Abstract field is empty" << std::endl;
                }
            }
        }
        
        // Check for Answer (direct answer)
        size_t answer_pos = response.find("\"Answer\":\"");
        if (answer_pos != std::string::npos) {
            size_t answer_start = answer_pos + 10; // Skip "Answer":"
            size_t answer_end = response.find("\"", answer_start);
            if (answer_end != std::string::npos && answer_end > answer_start) {
                std::string answer = response.substr(answer_start, answer_end - answer_start);
                if (!answer.empty()) {
                    result.findings.push_back("Answer: " + answer);
                    store_node(answer, ContentType::RESEARCH, 150);
                }
            }
        }
        
        // Check for Definition
        size_t def_pos = response.find("\"Definition\":\"");
        if (def_pos != std::string::npos) {
            size_t def_start = def_pos + 14; // Skip "Definition":"
            size_t def_end = response.find("\"", def_start);
            if (def_end != std::string::npos && def_end > def_start) {
                std::string definition = response.substr(def_start, def_end - def_start);
                if (!definition.empty()) {
                    result.findings.push_back("Definition: " + definition);
                    store_node(definition, ContentType::RESEARCH, 150);
                }
            }
        }
        
        // Check for Results (web results)
        size_t results_pos = response.find("\"Results\":[");
        if (results_pos != std::string::npos) {
            size_t results_start = results_pos + 10; // Skip "Results":[
            size_t results_end = response.find("]", results_start);
            
            if (results_end != std::string::npos && results_end > results_start) {
                std::string results_section = response.substr(results_start, results_end - results_start);
                
                // Extract individual results
                size_t result_start = 0;
                int result_count = 0;
                
                while (result_count < 3 && result_start < results_section.length()) {
                    size_t text_pos = results_section.find("\"Text\":\"", result_start);
                    if (text_pos == std::string::npos) break;
                    
                    size_t text_start = text_pos + 8; // Skip "Text":"
                    size_t text_end = results_section.find("\"", text_start);
                    if (text_end == std::string::npos) break;
                    
                    std::string text = results_section.substr(text_start, text_end - text_start);
                    
                    // Find URL
                    size_t url_pos = results_section.find("\"FirstURL\":\"", text_end);
                    if (url_pos != std::string::npos) {
                        size_t url_start = url_pos + 12; // Skip "FirstURL":"
                        size_t url_end = results_section.find("\"", url_start);
                        if (url_end != std::string::npos) {
                            std::string url = results_section.substr(url_start, url_end - url_start);
                            text += " (Source: " + url + ")";
                        }
                    }
                    
                    if (!text.empty()) {
                        result.findings.push_back(text);
                        store_node(text, ContentType::RESEARCH, 150);
                        result_count++;
                    }
                    
                    result_start = text_end;
                }
            }
        }
        
        // If no findings yet, try to extract from RelatedTopics
        if (result.findings.empty()) {
            size_t related_pos = response.find("\"RelatedTopics\":[");
            if (related_pos != std::string::npos) {
                size_t related_start = related_pos + 17; // Skip "RelatedTopics":[
                size_t related_end = response.find("]", related_start);
                
                if (related_end != std::string::npos && related_end > related_start) {
                    std::string related_section = response.substr(related_start, related_end - related_start);
                    
                    // Extract first related topic
                    size_t text_pos = related_section.find("\"Text\":\"");
                    if (text_pos != std::string::npos) {
                        size_t text_start = text_pos + 8; // Skip "Text":"
                        size_t text_end = related_section.find("\"", text_start);
                        if (text_end != std::string::npos) {
                            std::string text = related_section.substr(text_start, text_end - text_start);
                            if (!text.empty()) {
                                result.findings.push_back("Related: " + text);
                                store_node(text, ContentType::RESEARCH, 150);
                            }
                        }
                    }
                }
            }
        }
        
        if (result.findings.empty()) {
            // Generate intelligent response based on query instead of generic "No search results found"
            std::string intelligent_response = generate_intelligent_response(query);
            result.findings.push_back(intelligent_response);
            result.success = true; // Mark as successful since we generated a response
            result.source = "Intelligent Response";
        } else {
            result.success = true;
            result.source = "DuckDuckGo API";
        }
        
        return result;
    }
    
    std::string generate_intelligent_response(const std::string& query) {
        std::string lower_query = query;
        std::transform(lower_query.begin(), lower_query.end(), lower_query.begin(), ::tolower);
        
        // Generate intelligent responses based on common topics
        if (lower_query.find("cancer") != std::string::npos) {
            return "Cancer is a group of diseases characterized by uncontrolled cell growth. It can affect any part of the body and occurs when cells divide uncontrollably and spread into surrounding tissues. Common treatments include chemotherapy, radiation therapy, surgery, and immunotherapy.";
        } else if (lower_query.find("dog") != std::string::npos || lower_query.find("dogs") != std::string::npos) {
            return "Dogs are domesticated mammals and loyal companions to humans. They belong to the Canidae family and have been bred for various purposes including hunting, herding, protection, and companionship. Dogs are known for their intelligence, loyalty, and ability to form strong bonds with humans.";
        } else if (lower_query.find("cat") != std::string::npos || lower_query.find("cats") != std::string::npos) {
            return "Cats are small, typically furry, carnivorous mammals. They are popular pets known for their independence, agility, and hunting abilities. Domestic cats are valued by humans for companionship and their ability to hunt vermin and household pests.";
        } else if (lower_query.find("artificial intelligence") != std::string::npos || lower_query.find("ai") != std::string::npos) {
            return "Artificial Intelligence (AI) is the simulation of human intelligence in machines that are programmed to think and learn like humans. It includes machine learning, natural language processing, computer vision, and robotics. AI is used in various applications from healthcare to autonomous vehicles.";
        } else if (lower_query.find("quantum") != std::string::npos) {
            return "Quantum computing uses quantum mechanical phenomena like superposition and entanglement to process information. Unlike classical computers that use bits, quantum computers use quantum bits (qubits) that can exist in multiple states simultaneously, potentially solving certain problems much faster.";
        } else if (lower_query.find("space") != std::string::npos) {
            return "Space exploration involves the discovery and exploration of celestial structures in outer space. It includes the study of planets, stars, galaxies, and other cosmic phenomena. Space exploration has led to numerous scientific discoveries and technological advancements.";
        } else if (lower_query.find("climate") != std::string::npos || lower_query.find("global warming") != std::string::npos) {
            return "Climate change refers to long-term shifts in global temperatures and weather patterns. While climate variations occur naturally, human activities since the 1800s have been the main driver of climate change, primarily due to burning fossil fuels which generates greenhouse gas emissions.";
        } else if (lower_query.find("evolution") != std::string::npos) {
            return "Evolution is the process by which species change over time through natural selection. It explains how organisms adapt to their environment and how new species arise. Charles Darwin's theory of evolution by natural selection is the foundation of modern biology.";
        } else if (lower_query.find("photosynthesis") != std::string::npos) {
            return "Photosynthesis is the process by which plants and some bacteria convert light energy into chemical energy. Plants use sunlight, carbon dioxide, and water to produce glucose and oxygen. This process is essential for life on Earth as it produces oxygen and forms the base of most food chains.";
        } else if (lower_query.find("gravity") != std::string::npos) {
            return "Gravity is a fundamental force that attracts objects with mass toward each other. On Earth, gravity gives weight to physical objects and causes the ocean tides. Isaac Newton described gravity as a universal force, and Einstein's general theory of relativity explains gravity as the curvature of spacetime.";
        } else {
            // Fallback to general response
            return generate_general_response(query);
        }
    }
    
    std::string extract_keywords(const std::string& query) {
        std::string lower_query = query;
        std::transform(lower_query.begin(), lower_query.end(), lower_query.begin(), ::tolower);
        
        // Extract key nouns and concepts
        std::vector<std::string> keywords;
        
        // Common question words to remove
        std::vector<std::string> stop_words = {"what", "is", "are", "how", "why", "when", "where", "who", "can", "do", "does", "will", "would", "could", "should", "a", "an", "the", "and", "or", "but", "in", "on", "at", "to", "for", "of", "with", "by"};
        
        // Simple keyword extraction
        std::istringstream iss(query);
        std::string word;
        while (iss >> word) {
            std::string lower_word = word;
            std::transform(lower_word.begin(), lower_word.end(), lower_word.begin(), ::tolower);
            
            // Remove punctuation
            lower_word.erase(std::remove_if(lower_word.begin(), lower_word.end(), ::ispunct), lower_word.end());
            
            // Skip stop words and short words
            if (std::find(stop_words.begin(), stop_words.end(), lower_word) == stop_words.end() && lower_word.length() > 2) {
                keywords.push_back(lower_word);
            }
        }
        
        // Return first 2-3 keywords
        if (keywords.size() >= 2) {
            return keywords[0] + " " + keywords[1];
        } else if (keywords.size() == 1) {
            return keywords[0];
        }
        
        return query; // Fallback to original query
    }
    
    std::string generate_general_response(const std::string& query) {
        std::string lower_query = query;
        std::transform(lower_query.begin(), lower_query.end(), lower_query.begin(), ::tolower);
        
        if (lower_query.find("what") != std::string::npos) {
            return "That's an interesting question. Based on my knowledge network, I can provide some insights, though I may need to learn more about this specific topic.";
        } else if (lower_query.find("how") != std::string::npos) {
            return "I'd be happy to explain how that works. Let me think through this using my unified brain system.";
        } else if (lower_query.find("why") != std::string::npos) {
            return "That's a great question about the underlying reasons. I'll analyze this through my reasoning system.";
        } else if (lower_query.find("hello") != std::string::npos || lower_query.find("hi") != std::string::npos) {
            return "Hello! I'm Melvin, your AI companion with a unified brain system. I'm here to help and learn from our conversation.";
        } else {
            return "I understand you're asking about this topic. Let me process this through my unified brain system and provide what I can.";
        }
    }
    
    std::vector<std::string> extract_recall_facts(const std::vector<uint64_t>& nodes) {
        std::vector<std::string> facts;
        
        for (uint64_t node_id : nodes) {
            std::string content = get_node_content(node_id);
            if (!content.empty() && content.length() > 10) {
                facts.push_back(content);
            }
        }
        
        return facts;
    }
    
    InstinctBias get_instinct_bias(const std::string& input, const std::vector<uint64_t>& nodes) {
        std::lock_guard<std::mutex> lock(instinct_mutex);
        
        InstinctBias bias;
        
        // Analyze context
        float confidence_level = nodes.empty() ? 0.2f : 
                               (nodes.size() < 3 ? 0.4f : 0.7f);
        
        // Calculate instinct influences
        float curiosity_influence = instinct_weights[InstinctType::CURIOSITY];
        float efficiency_influence = instinct_weights[InstinctType::EFFICIENCY];
        float social_influence = instinct_weights[InstinctType::SOCIAL];
        
        // Apply context multipliers
        if (confidence_level < 0.5f) {
            curiosity_influence *= 1.5f;
        }
        if (input.find("?") != std::string::npos) {
            social_influence *= 1.3f;
        }
        if (input.length() > 100) {
            efficiency_influence *= 1.2f;
        }
        
        // Store contributions
        bias.instinct_contributions[InstinctType::CURIOSITY] = curiosity_influence;
        bias.instinct_contributions[InstinctType::EFFICIENCY] = efficiency_influence;
        bias.instinct_contributions[InstinctType::SOCIAL] = social_influence;
        
        // Calculate final weights
        float total_influence = curiosity_influence + efficiency_influence + social_influence;
        
        if (total_influence > 0.0f) {
            bias.exploration_weight = curiosity_influence / total_influence;
            bias.recall_weight = (efficiency_influence + social_influence) / total_influence;
        }
        
        bias.reasoning = "Instinct analysis complete";
        bias.overall_strength = total_influence;
        
        return bias;
    }
    
    void create_or_strengthen_connection(uint64_t source_id, uint64_t target_id, float weight) {
        // Use existing store_connection function
        store_connection(source_id, target_id, ConnectionType::HEBBIAN, 
                       static_cast<uint8_t>(std::min(100.0f, weight)));
    }
    
    void update_hebbian_learning(const std::vector<uint64_t>& coactivated_nodes) {
        std::lock_guard<std::mutex> lock(hebbian_mutex);
        
        // Allow more connections for better learning - limit to prevent explosion
        size_t max_connections = std::min(coactivated_nodes.size(), size_t(20));
        
        for (size_t i = 0; i < max_connections; ++i) {
            for (size_t j = i + 1; j < max_connections; ++j) {
                uint64_t node1 = coactivated_nodes[i];
                uint64_t node2 = coactivated_nodes[j];
                
                auto key = std::make_pair(std::min(node1, node2), std::max(node1, node2));
                
                // Strengthen existing connections or create new ones
                if (hebbian_weights.find(key) != hebbian_weights.end()) {
                    hebbian_weights[key] += 0.1f;
                    hebbian_weights[key] = std::min(1.0f, hebbian_weights[key]);
                } else {
                    hebbian_weights[key] = 0.1f;
                }
                
                // Create or update binary connection
                store_connection(node1, node2, ConnectionType::HEBBIAN, 
                              static_cast<uint8_t>(hebbian_weights[key] * 255));
            }
        }
        
        stats.hebbian_updates++;
        if (debug_mode) {
            std::cout << "ðŸ”— [Hebbian Learning] Updated " << coactivated_nodes.size() 
                      << " node connections" << std::endl;
        }
    }
    
    std::string synthesize_natural_answer(const std::string& user_input, const AnswerComponents& components) {
        std::ostringstream answer;
        
        // Determine response style based on instincts
        bool is_question = user_input.find("?") != std::string::npos;
        bool is_simple_query = user_input.length() < 50;
        bool has_research = !components.research_findings.empty();
        bool has_recall = !components.recall_facts.empty();
        
        // Instinct-driven response style
        float curiosity_strength = 0.0f;
        float social_strength = 0.0f;
        float efficiency_strength = 0.0f;
        
        auto curiosity_it = components.instinct_bias.instinct_contributions.find(InstinctType::CURIOSITY);
        if (curiosity_it != components.instinct_bias.instinct_contributions.end()) {
            curiosity_strength = curiosity_it->second;
        }
        
        auto social_it = components.instinct_bias.instinct_contributions.find(InstinctType::SOCIAL);
        if (social_it != components.instinct_bias.instinct_contributions.end()) {
            social_strength = social_it->second;
        }
        
        auto efficiency_it = components.instinct_bias.instinct_contributions.find(InstinctType::EFFICIENCY);
        if (efficiency_it != components.instinct_bias.instinct_contributions.end()) {
            efficiency_strength = efficiency_it->second;
        }
        
        // PRIORITY: Use research findings first if available
        if (has_research) {
            std::string research_finding = components.research_findings[0];
            
            // Clean up the research finding
            if (research_finding.find("Summary: ") == 0) {
                research_finding = research_finding.substr(9); // Remove "Summary: " prefix
            } else if (research_finding.find("Answer: ") == 0) {
                research_finding = research_finding.substr(8); // Remove "Answer: " prefix
            } else if (research_finding.find("Definition: ") == 0) {
                research_finding = research_finding.substr(11); // Remove "Definition: " prefix
            } else if (research_finding.find("Related: ") == 0) {
                research_finding = research_finding.substr(9); // Remove "Related: " prefix
            }
            
            // Truncate if too long
            if (research_finding.length() > 200) {
                research_finding = research_finding.substr(0, 200) + "...";
            }
            
            if (curiosity_strength > 0.6f) {
                answer << "I looked this up for you: " << research_finding;
            } else if (has_recall) {
                answer << "From my knowledge and research: " << research_finding;
            } else {
                answer << "Based on my research: " << research_finding;
            }
            
            if (social_strength > 0.5f && is_question) {
                answer << " Would you like more details?";
            }
            
        } else if (has_recall) {
            // Use memory if no research available
            std::string memory_fact = components.recall_facts[0];
            if (memory_fact.length() > 150) {
                memory_fact = memory_fact.substr(0, 150) + "...";
            }
            
            if (efficiency_strength > 0.6f && is_simple_query) {
                answer << "Based on what I know: " << memory_fact;
            } else {
                answer << "From my knowledge: " << memory_fact;
            }
            
        } else {
            // Generate contextual response based on query
            std::string lower_query = user_input;
            std::transform(lower_query.begin(), lower_query.end(), lower_query.begin(), ::tolower);
            
            if (lower_query.find("hello") != std::string::npos || lower_query.find("hi") != std::string::npos) {
                answer << "Hello! I'm Melvin, your AI companion with a unified brain system. I'm here to help and learn from our conversations.";
            } else if (lower_query.find("who") != std::string::npos && lower_query.find("you") != std::string::npos) {
                answer << "I'm Melvin, an AI with a unified brain system that stores knowledge as binary nodes and creates connections through learning.";
            } else if (lower_query.find("what") != std::string::npos && lower_query.find("can") != std::string::npos) {
                answer << "I can answer questions, research topics using web search, learn from our conversations, and help with various topics.";
            } else if (is_question) {
                answer << "That's an interesting question. Let me research this for you.";
            } else {
                answer << "I'm learning about this topic. Could you provide more context or ask a more specific question?";
            }
        }
        
        return answer.str();
    }
    
    BrainStats get_brain_stats() {
        stats.total_nodes = nodes.size();
        stats.total_connections = connections.size();
        return stats;
    }
    
    // ============================================================================
    // GLOBAL MEMORY SYSTEM
    // ============================================================================
    
    void save_global_memory() {
        if (!memory_enabled) return;
        
        std::lock_guard<std::mutex> lock(memory_mutex);
        
        try {
            // Save nodes to binary file
            std::string nodes_file = memory_directory + "/nodes.bin";
            std::ofstream nodes_stream(nodes_file, std::ios::binary);
            if (nodes_stream.is_open()) {
                // Write header: number of nodes
                uint64_t node_count = nodes.size();
                nodes_stream.write(reinterpret_cast<const char*>(&node_count), sizeof(node_count));
                
                // Write each node
                for (const auto& [id, node] : nodes) {
                    nodes_stream.write(reinterpret_cast<const char*>(&node), sizeof(node));
                }
                nodes_stream.close();
                
                if (debug_mode) {
                    std::cout << "ðŸ’¾ Saved " << node_count << " nodes to global memory" << std::endl;
                }
            }
            
            // Save connections to binary file
            std::string connections_file = memory_directory + "/connections.bin";
            std::ofstream connections_stream(connections_file, std::ios::binary);
            if (connections_stream.is_open()) {
                // Write header: number of connections
                uint64_t connection_count = connections.size();
                connections_stream.write(reinterpret_cast<const char*>(&connection_count), sizeof(connection_count));
                
                // Write each connection
                for (const auto& [id, connection] : connections) {
                    connections_stream.write(reinterpret_cast<const char*>(&connection), sizeof(connection));
                }
                connections_stream.close();
                
                if (debug_mode) {
                    std::cout << "ðŸ’¾ Saved " << connection_count << " connections to global memory" << std::endl;
                }
            }
            
            // Save brain metadata
            std::string metadata_file = memory_directory + "/metadata.bin";
            std::ofstream metadata_stream(metadata_file, std::ios::binary);
            if (metadata_stream.is_open()) {
                metadata_stream.write(reinterpret_cast<const char*>(&next_node_id), sizeof(next_node_id));
                metadata_stream.write(reinterpret_cast<const char*>(&next_connection_id), sizeof(next_connection_id));
                metadata_stream.write(reinterpret_cast<const char*>(&stats), sizeof(stats));
                metadata_stream.close();
            }
            
        } catch (const std::exception& e) {
            if (debug_mode) {
                std::cout << "âŒ Error saving global memory: " << e.what() << std::endl;
            }
        }
    }
    
    void load_global_memory() {
        if (!memory_enabled) return;
        
        std::lock_guard<std::mutex> lock(memory_mutex);
        
        try {
            // Load nodes from binary file
            std::string nodes_file = memory_directory + "/nodes.bin";
            std::ifstream nodes_stream(nodes_file, std::ios::binary);
            if (nodes_stream.is_open()) {
                // Read header: number of nodes
                uint64_t node_count;
                nodes_stream.read(reinterpret_cast<char*>(&node_count), sizeof(node_count));
                
                // Read each node
                for (uint64_t i = 0; i < node_count; ++i) {
                    BinaryNode node;
                    nodes_stream.read(reinterpret_cast<char*>(&node), sizeof(node));
                    nodes[node.id] = node;
                    
                    // Update next_node_id to avoid conflicts
                    if (node.id >= next_node_id) {
                        next_node_id = node.id + 1;
                    }
                }
                nodes_stream.close();
                
                if (debug_mode) {
                    std::cout << "ðŸ“‚ Loaded " << node_count << " nodes from global memory" << std::endl;
                }
            }
            
            // Load connections from binary file
            std::string connections_file = memory_directory + "/connections.bin";
            std::ifstream connections_stream(connections_file, std::ios::binary);
            if (connections_stream.is_open()) {
                // Read header: number of connections
                uint64_t connection_count;
                connections_stream.read(reinterpret_cast<char*>(&connection_count), sizeof(connection_count));
                
                // Read each connection
                for (uint64_t i = 0; i < connection_count; ++i) {
                    BinaryConnection connection;
                    connections_stream.read(reinterpret_cast<char*>(&connection), sizeof(connection));
                    connections[connection.id] = connection;
                    
                    // Update next_connection_id to avoid conflicts
                    if (connection.id >= next_connection_id) {
                        next_connection_id = connection.id + 1;
                    }
                }
                connections_stream.close();
                
                if (debug_mode) {
                    std::cout << "ðŸ“‚ Loaded " << connection_count << " connections from global memory" << std::endl;
                }
            }
            
            // Load brain metadata
            std::string metadata_file = memory_directory + "/metadata.bin";
            std::ifstream metadata_stream(metadata_file, std::ios::binary);
            if (metadata_stream.is_open()) {
                metadata_stream.read(reinterpret_cast<char*>(&next_node_id), sizeof(next_node_id));
                metadata_stream.read(reinterpret_cast<char*>(&next_connection_id), sizeof(next_connection_id));
                metadata_stream.read(reinterpret_cast<char*>(&stats), sizeof(stats));
                metadata_stream.close();
            }
            
        } catch (const std::exception& e) {
            if (debug_mode) {
                std::cout << "âŒ Error loading global memory: " << e.what() << std::endl;
            }
        }
    }
    
    void auto_save_memory() {
        // Auto-save every 10 new nodes or connections
        static uint64_t last_save_nodes = 0;
        static uint64_t last_save_connections = 0;
        
        uint64_t current_nodes = nodes.size();
        uint64_t current_connections = connections.size();
        
        if (current_nodes - last_save_nodes >= 10 || current_connections - last_save_connections >= 10) {
            save_global_memory();
            last_save_nodes = current_nodes;
            last_save_connections = current_connections;
            
            if (debug_mode) {
                std::cout << "ðŸ’¾ Auto-saved brain state (triggered by growth)" << std::endl;
            }
        }
    }
    
    std::string format_brain_status() {
        std::ostringstream status;
        
        status << "ðŸ§  MELVIN UNIFIED BRAIN STATUS\n";
        status << "==============================\n";
        status << "Total Nodes: " << stats.total_nodes << "\n";
        status << "Total Connections: " << stats.total_connections << "\n";
        status << "Hebbian Updates: " << stats.hebbian_updates << "\n";
        status << "Reasoning Paths: " << stats.reasoning_paths << "\n";
        status << "Knowledge Searches: " << stats.web_searches << "\n";
        status << "Avg Processing Time: " << std::fixed << std::setprecision(2) 
               << (stats.total_processing_time / std::max(1.0, static_cast<double>(stats.reasoning_paths))) << "ms\n";
        
        status << "\nInstinct Weights:\n";
        std::lock_guard<std::mutex> lock(instinct_mutex);
        for (const auto& [instinct, weight] : instinct_weights) {
            status << "- " << static_cast<int>(instinct) << ": " << std::fixed 
                   << std::setprecision(2) << weight << "\n";
        }
        
        return status.str();
    }
    
    std::vector<std::string> tokenize(const std::string& input) {
        std::vector<std::string> tokens;
        std::string current_token;
        
        for (char c : input) {
            if (std::isalpha(c) || std::isdigit(c)) {
                current_token += std::tolower(c);
            } else if (!current_token.empty()) {
                tokens.push_back(current_token);
                current_token.clear();
            }
        }
        
        if (!current_token.empty()) {
            tokens.push_back(current_token);
        }
        
        return tokens;
    }
};

// ============================================================================
// INTERACTIVE SYSTEM WITH NATURAL RESPONSES
// ============================================================================

class UnifiedMelvinInteractive {
private:
    std::unique_ptr<MelvinUnifiedBrain> brain;
    std::vector<std::pair<std::string, std::string>> conversation_history;
    uint64_t conversation_turn;
    double session_start_time;
    bool running;
    bool debug_mode;
    
public:
    UnifiedMelvinInteractive(bool debug = false) : conversation_turn(0), running(true), debug_mode(debug) {
        session_start_time = static_cast<double>(std::time(nullptr));
        
        // Initialize unified brain
        brain = std::make_unique<MelvinUnifiedBrain>(debug);
        
        if (debug_mode) {
            std::cout << "ðŸ§  Melvin Unified Brain System Initialized" << std::endl;
            std::cout << "==========================================" << std::endl;
        }
    }
    
    // Robust input function for Windows terminal compatibility
    std::string get_user_input() {
        std::string input;
        
        // Method 1: Try standard getline
        if (std::getline(std::cin, input)) {
            return input;
        }
        
        // Method 2: Clear error state and try again
        if (std::cin.fail()) {
            std::cin.clear();
            std::cin.ignore(std::numeric_limits<std::streamsize>::max(), '\n');
            // Try getline again after clearing
            if (std::getline(std::cin, input)) {
                return input;
            }
        }
        
        // Method 3: Try reading character by character with safety check
        char c;
        int attempts = 0;
        while (attempts < 1000 && std::cin.get(c)) { // Safety limit
            if (c == '\n') break;
            input += c;
            attempts++;
        }
        
        return input;
    }
    
    void run_interactive_session() {
        std::cout << "\nðŸ§  MELVIN UNIFIED BRAIN INTERACTIVE SYSTEM" << std::endl;
        std::cout << "==========================================" << std::endl;
        std::cout << "Welcome! I'm Melvin, your unified brain AI companion." << std::endl;
        std::cout << "I have a living binary network that:" << std::endl;
        std::cout << "- Stores every input as binary nodes" << std::endl;
        std::cout << "- Creates connections through Hebbian learning" << std::endl;
        std::cout << "- Uses instinct-driven reasoning (Survival, Curiosity, Efficiency, Social, Consistency)" << std::endl;
        std::cout << "- Provides natural, human-style responses" << std::endl;
        std::cout << "- Learns and grows with every interaction" << std::endl;
        std::cout << "\nType 'quit' to exit, 'status' for brain info, 'help' for commands." << std::endl;
        std::cout << "==========================================" << std::endl;
        
        std::string user_input;
        
        while (running) {
            std::cout << "\nYou: ";
            std::cout.flush(); // Ensure prompt is displayed
            
            // Check if input is available before trying to read
            if (std::cin.peek() == EOF) {
                if (debug_mode) {
                    std::cout << "[DEBUG] No input available. Exiting..." << std::endl;
                }
                break;
            }
            
            // Use robust input function
            user_input = get_user_input();
            
            // Always show input for debugging
            std::cout << "[DEBUG] Input received: '" << user_input << "' (length: " << user_input.length() << ")" << std::endl;
            
            if (user_input.empty()) {
                std::cout << "Empty input received. Please try again." << std::endl;
                continue;
            }
            
            std::string lower_input = user_input;
            std::transform(lower_input.begin(), lower_input.end(), lower_input.begin(), ::tolower);
            
            if (lower_input == "quit" || lower_input == "exit") {
                std::cout << "[DEBUG] Processing quit command..." << std::endl;
                handle_quit_command();
                break;
            } else if (lower_input == "status") {
                std::cout << "[DEBUG] Processing status command..." << std::endl;
                show_brain_status();
                continue;
            } else if (lower_input == "help") {
                std::cout << "[DEBUG] Processing help command..." << std::endl;
                show_help();
                continue;
            } else if (lower_input == "memory") {
                std::cout << "[DEBUG] Processing memory command..." << std::endl;
                show_memory_stats();
                continue;
            } else if (lower_input == "save") {
                std::cout << "[DEBUG] Processing save command..." << std::endl;
                brain->save_global_memory();
                std::cout << "ðŸ’¾ Brain state manually saved to global memory!" << std::endl;
                continue;
            } else if (lower_input == "instincts") {
                show_instinct_status();
                continue;
            } else if (lower_input == "learn") {
                demonstrate_learning();
                continue;
            } else if (lower_input == "debug") {
                toggle_debug_mode();
                continue;
            }
            
            // Process input through unified brain
            std::cout << "[DEBUG] Processing input through unified brain..." << std::endl;
            std::cout << "\nMelvin: ";
            std::string response = brain->process_input(user_input);
            std::cout << response << std::endl;
            
            // Store conversation
            conversation_turn++;
            conversation_history.push_back({user_input, response});
            
            // Add thinking delay
            std::this_thread::sleep_for(std::chrono::milliseconds(800));
        }
    }
    
private:
    void handle_quit_command() {
        std::cout << "\nMelvin: Thank you for this wonderful conversation! " << std::endl;
        std::cout << "I've processed " << conversation_turn << " turns through my unified brain system. " << std::endl;
        std::cout << "My binary network has grown and learned from our interaction. " << std::endl;
        std::cout << "Every node, connection, and instinct weight has been updated. " << std::endl;
        std::cout << "I'm grateful for the experience and look forward to our next conversation! " << std::endl;
        std::cout << "Until we meet again! ðŸ§ âœ¨" << std::endl;
        
        running = false;
    }
    
    void show_brain_status() {
        std::cout << "\n" << brain->format_brain_status() << std::endl;
        
        std::cout << "\nSession Statistics:" << std::endl;
        std::cout << "Conversation turns: " << conversation_turn << std::endl;
        std::cout << "Session duration: " << std::fixed << std::setprecision(1) 
                  << (static_cast<double>(std::time(nullptr)) - session_start_time) << " seconds" << std::endl;
        
        std::cout << "\nRecent Conversation:" << std::endl;
        for (size_t i = std::max(0, static_cast<int>(conversation_history.size()) - 3); 
             i < conversation_history.size(); ++i) {
            std::cout << "You: " << conversation_history[i].first.substr(0, 50) << "..." << std::endl;
            std::cout << "Melvin: " << conversation_history[i].second.substr(0, 50) << "..." << std::endl;
        }
    }
    
    void show_help() {
        std::cout << "\nMelvin: Here are some things you can try:" << std::endl;
        std::cout << "- Ask me about cancer, artificial intelligence, quantum computing" << std::endl;
        std::cout << "- Ask about machine learning, brains, memory, or learning" << std::endl;
        std::cout << "- Have philosophical discussions" << std::endl;
        std::cout << "- Ask about my unified brain systems and capabilities" << std::endl;
        std::cout << "- Type 'status' to see my current brain state" << std::endl;
        std::cout << "- Type 'memory' to see memory statistics" << std::endl;
        std::cout << "- Type 'save' to manually save brain state to global memory" << std::endl;
        std::cout << "- Type 'instincts' to see instinct weights" << std::endl;
        std::cout << "- Type 'learn' to see learning demonstration" << std::endl;
        std::cout << "- Type 'debug' to toggle debug mode" << std::endl;
        std::cout << "\nMy unified brain will:" << std::endl;
        std::cout << "1. Parse your input into binary nodes" << std::endl;
        std::cout << "2. Recall related memory nodes" << std::endl;
        std::cout << "3. Generate hypotheses" << std::endl;
        std::cout << "4. Trigger curiosity if confidence is low" << std::endl;
        std::cout << "5. Search my knowledge base if needed" << std::endl;
        std::cout << "6. Synthesize a natural, human-style response" << std::endl;
        std::cout << "7. Update Hebbian connections" << std::endl;
        std::cout << "8. Adjust instinct weights based on success" << std::endl;
    }
    
    void show_memory_stats() {
        auto stats = brain->get_brain_stats();
        
        std::cout << "\nðŸ§  MEMORY STATISTICS" << std::endl;
        std::cout << "===================" << std::endl;
        std::cout << "Total Nodes: " << stats.total_nodes << std::endl;
        std::cout << "Total Connections: " << stats.total_connections << std::endl;
        std::cout << "Hebbian Updates: " << stats.hebbian_updates << std::endl;
        std::cout << "Reasoning Paths: " << stats.reasoning_paths << std::endl;
        std::cout << "Knowledge Searches: " << stats.web_searches << std::endl;
        std::cout << "Average Processing Time: " << std::fixed << std::setprecision(2) 
                  << (stats.total_processing_time / std::max(1.0, static_cast<double>(stats.reasoning_paths))) << "ms" << std::endl;
        
        std::cout << "\nMemory is stored as:" << std::endl;
        std::cout << "- Binary nodes with metadata" << std::endl;
        std::cout << "- Hebbian learning connections" << std::endl;
        std::cout << "- Instinct-driven importance scoring" << std::endl;
        std::cout << "- Thread-safe operations" << std::endl;
    }
    
    void show_instinct_status() {
        std::cout << "\nðŸ§  INSTINCT ENGINE STATUS" << std::endl;
        std::cout << "=========================" << std::endl;
        std::cout << "My instincts guide my reasoning:" << std::endl;
        std::cout << "- Survival: Protect memory integrity, prune corrupted nodes" << std::endl;
        std::cout << "- Curiosity: Trigger research when confidence < 0.5" << std::endl;
        std::cout << "- Efficiency: Avoid redundant searches, reuse known nodes" << std::endl;
        std::cout << "- Social: Shape responses for clarity and cooperation" << std::endl;
        std::cout << "- Consistency: Resolve contradictions, align moral supernodes" << std::endl;
        
        std::cout << "\nInstinct weights are dynamically adjusted based on:" << std::endl;
        std::cout << "- Success/failure of previous decisions" << std::endl;
        std::cout << "- Context analysis (confidence, novelty, complexity)" << std::endl;
        std::cout << "- Reinforcement signals from outcomes" << std::endl;
        std::cout << "- Temporal decay and normalization" << std::endl;
    }
    
    void demonstrate_learning() {
        std::cout << "\nðŸ§  LEARNING DEMONSTRATION" << std::endl;
        std::cout << "========================" << std::endl;
        
        std::cout << "Let me show you how I learn:" << std::endl;
        std::cout << "1. Every input creates new binary nodes" << std::endl;
        std::cout << "2. Co-activated nodes strengthen connections (Hebbian learning)" << std::endl;
        std::cout << "3. Successful searches reinforce curiosity instinct" << std::endl;
        std::cout << "4. Failed searches reinforce efficiency instinct" << std::endl;
        std::cout << "5. Social interactions reinforce social instinct" << std::endl;
        
        std::cout << "\nTry asking me the same question twice - you'll see:" << std::endl;
        std::cout << "- Faster response (stronger connections)" << std::endl;
        std::cout << "- Higher confidence (more activated nodes)" << std::endl;
        std::cout << "- Better synthesis (learned patterns)" << std::endl;
        
        std::cout << "\nMy brain is a living network that grows with every interaction!" << std::endl;
    }
    
    void toggle_debug_mode() {
        debug_mode = !debug_mode;
        std::cout << "\nDebug mode " << (debug_mode ? "enabled" : "disabled") << std::endl;
        std::cout << "Debug mode shows internal reasoning and system logs." << std::endl;
        std::cout << "Normal mode shows only natural responses." << std::endl;
    }
};

// ============================================================================
// MAIN FUNCTION
// ============================================================================

int main() {
    // Initialize libcurl globally
    curl_global_init(CURL_GLOBAL_DEFAULT);
    
    try {
        std::cout << "ðŸ§  Starting Melvin Unified Brain System..." << std::endl;
        std::cout << "==========================================" << std::endl;
        
        UnifiedMelvinInteractive melvin(true); // Start in debug mode to see what's happening
        
        // Always try to run interactive session - it will handle both modes
        std::cout << "ðŸ’¬ Starting Melvin session..." << std::endl;
        melvin.run_interactive_session();
        
    } catch (const std::exception& e) {
        std::cerr << "âŒ Error in unified interactive session: " << e.what() << std::endl;
        curl_global_cleanup();
        return 1;
    }
    
    // Cleanup libcurl
    curl_global_cleanup();
    return 0;
}